model:
  kind: gptspeech
  seqlen: 2048
  n_quantizers: 2
  max_duration: 8.0
train:
  batch_size: 8
  checkpoint_every: 250
  dataset: libritts
  gradient_accumulation_steps: 16
  lr: 0.0003
  seed: 42
  val_every: 250