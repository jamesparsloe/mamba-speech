model:
  n_quantizers: 2
  d_model: 768
  n_layer: 24
train:
  batch_size: 8
  checkpoint_every: 250
  dataset: libritts
  gradient_accumulation_steps: 16
  lr: 0.0001
  seed: 42
  val_every: 250