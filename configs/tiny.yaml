model:
  kind: mambaspeech
  n_quantizers: 2
train:
  batch_size: 16
  checkpoint_every: 250
  dataset: libritts
  gradient_accumulation_steps: 8
  lr: 0.0003
  seed: 42
  val_every: 250